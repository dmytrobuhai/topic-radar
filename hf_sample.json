[
  {
    "source": "HF Papers",
    "hf_url": "https://huggingface.co/papers/2508.00399",
    "hf_slug": "2508.00399",
    "title": "iSafetyBench: A video-language benchmark for safety in industrial\n  environment",
    "summary": "Recent advances in vision-language models (VLMs) have enabled impressive\ngeneralization across diverse video understanding tasks under zero-shot\nsettings. However, their capabilities in high-stakes industrial domains-where\nrecognizing both routine operations and safety-critical anomalies is\nessential-remain largely underexplored. To address this gap, we introduce\niSafetyBench, a new video-language benchmark specifically designed to evaluate\nmodel performance in industrial environments across both normal and hazardous\nscenarios. iSafetyBench comprises 1,100 video clips sourced from real-world\nindustrial settings, annotated with open-vocabulary, multi-label action tags\nspanning 98 routine and 67 hazardous action categories. Each clip is paired\nwith multiple-choice questions for both single-label and multi-label\nevaluation, enabling fine-grained assessment of VLMs in both standard and\nsafety-critical contexts. We evaluate eight state-of-the-art video-language\nmodels under zero-shot conditions. Despite their strong performance on existing\nvideo benchmarks, these models struggle with iSafetyBench-particularly in\nrecognizing hazardous activities and in multi-label scenarios. Our results\nreveal significant performance gaps, underscoring the need for more robust,\nsafety-aware multimodal models for industrial applications. iSafetyBench\nprovides a first-of-its-kind testbed to drive progress in this direction. The\ndataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.",
    "date": "2025-08-01",
    "authors": "3 authors",
    "links": {
      "abs": "https://huggingface.co/papers/2508.00399",
      "arxiv": "https://arxiv.org/abs/2508.00399",
      "pdf": "https://arxiv.org/pdf/2508.00399.pdf"
    },
    "tags": [],
    "confidence": 30.63516929658717
  },
  {
    "source": "HF Papers",
    "hf_url": "https://huggingface.co/papers/2507.12414",
    "hf_slug": "2507.12414",
    "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models",
    "summary": "Training of autonomous driving systems requires extensive datasets with\nprecise annotations to attain robust performance. Human annotations suffer from\nimperfections, and multiple iterations are often needed to produce high-quality\ndatasets. However, manually reviewing large datasets is laborious and\nexpensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)\nframework and investigate the utilization of Vision-Language Models (VLMs) to\nautomatically identify erroneous annotations in vision datasets, thereby\nenabling users to eliminate these errors and enhance data quality. We validate\nour approach using the KITTI and nuImages datasets, which contain object\ndetection benchmarks for autonomous driving. To test the effectiveness of\nAutoVDC, we create dataset variants with intentionally injected erroneous\nannotations and observe the error detection rate of our approach. Additionally,\nwe compare the detection rates using different VLMs and explore the impact of\nVLM fine-tuning on our pipeline. The results demonstrate our method's high\nperformance in error detection and data cleaning experiments, indicating its\npotential to significantly improve the reliability and accuracy of large-scale\nproduction datasets in autonomous driving.",
    "date": "2025-07-16",
    "authors": "8 authors",
    "links": {
      "abs": "https://huggingface.co/papers/2507.12414",
      "arxiv": "https://arxiv.org/abs/2507.12414",
      "pdf": "https://arxiv.org/pdf/2507.12414.pdf"
    },
    "tags": [],
    "confidence": 27.4654364435989
  },
  {
    "source": "HF Papers",
    "hf_url": "https://huggingface.co/papers/2507.01955",
    "hf_slug": "2507.01955",
    "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation\n  Models on Standard Computer Vision Tasks",
    "summary": null,
    "date": "2025-07-15",
    "authors": "6 authors",
    "links": {
      "abs": "https://huggingface.co/papers/2507.01955",
      "arxiv": "https://arxiv.org/abs/2507.01955",
      "pdf": "https://arxiv.org/pdf/2507.01955.pdf"
    },
    "tags": [],
    "confidence": 33.26088017441391
  }
]